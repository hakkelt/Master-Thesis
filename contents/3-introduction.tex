\chapter{Introduction}\label{chapter:introduction}

While the fast evolution of technology profoundly changed today's medicine, unarguably the medical imaging is of the fields which profited the most of the computation power recently became available. And as X-rays revolutionized medical treatments in the beginning of the 20th century, after its discovery by Wilhelm Conrad Röntgen,  the appearance of computer-aided imaging techniques such as computer tomography (CT), diagnostic ultrasonography, positron emission tomography (PET), and magnetic resonance imaging (MRI) opened a new horizon drastically increasing the resolution, allowing 3D imaging, providing reliable dynamic recordings, and enhancing images by automated post-processing \cite{mri_picturetoproton}. In the recent decades radiology evolved to be an interdisciplinary field involving, for instance, molecular biology, nuclear physics, applied mathematics, and computer science besides the classical medical fields such as anatomy, angiology, and cardiology.

\section{Magnetic Resonance Imaging}

In particular, MRI has revolutionized medical imaging and diagnostic process as we know it. Its versatility makes it fit a wide range of use cases. Compared to other imaging technologies, MRI demonstrates important advantages in many cases.
\begin{itemize}
    \item In contrast to X-ray, MRI doesn't use any ionizing radiation, and hence it is totally harmless to the patient. Also, MRI has enhanced resolution for soft tissues compared to any other modality, in particular neural tissue,  while X-rays are rather used for diagnosing bone degeneration, dislocation, fracture or some tissue infection. Furthermore, MRI allows 3D scans. Nevertheless, MRI scanners are slow and expensive compare to X-ray scan machines and therefore hardware and software improvements are necessary for its wider use.
    \item As CT scanning is based on X-rays, it shares the downside with X-rays, doctors need to evaluate the possible benefits of the scan and decide if it outweighs the potential complications of exposure to ionizing radiations. MRI, however, elicit this problem, although at the price of a elongated imaging process. Comparing the medical problems where these technologies are used, one can conclude that CT scan is very helpful in diagnosing severe injuries of the chest, head, spine or abdomen, particularly fractures, and it is commonly used to localize tumors. MRI often performs better at diagnosing problems in the joints, soft tissues, ligaments and tendons. When available, doctors use it frequently to scan the spine, brain, muscles, neck, breasts, and abdomen.
    \item MRI still does not have the portability, low cost, and real-time imaging speed without any harmful radiation of ultrasound technology, but ultrasound is mostly limited to 2D imaging (although 3D imaging is possible), have trouble penetrating bone, and even in absence of bone the depth of penetration is limited depending on the frequency of imaging. This is not the case for MRI technology.
    \item PET scans are particularly useful for functional imaging. For instance, it is used for identification of lapses in cognitive function, examination of cardiac failures, cancer screening and diagnosis, and finding an infection.  Nevertheless, PET image acquisition is even longer than MRI (especially, if we consider also the time while patients wait for the tracer to reach the targeted organ), it uses a radioactive substance as tracer, and it cannot scan tissues not absorbing the tracer making the localization of the source of the signal infeasible without any additional information. In order to solve the latter described limitation, one particularly promising combination is the join use of PET and MRI. This illustrates that MRI is a fundamental technology not only by itself but also when combined with other imaging modalities.
\end{itemize}
To sum up, MRI is a strong competitor to other imaging technologies, but it also have weaknesses, of which the costs and scanning time are the most remarkable. There are many methods to speed up measurements as it will be discussed later, but the construction cost and the hardware constraints limit the applicability of these efforts. The problem of slowness is even more apparent in case of dynamic images as motion of organs (e.g. heart or lung) can drastically degrade the image quality. To overcome that issue, mathematical solutions developed in the last twenty years such as \emph{parallel imaging} and \emph{compressive sensing} made high-resolution and fast images possible. This thesis concerns the second of these two powerful mathematical ideas.

\section{Compressed Sensing}

Among the many software techniques invented to improve image quality, compressed sensing (also known as compressive sensing, compressed sampling, and compressive sampling) is inevitably is one of the most impactful theoretical construction, introduced by Donoho, Candès, Romberg, and Tao in 2004~\cite{candes_robust_2006, donoho_compressed_2006, candes_nearoptimal_2006}. Such importance can be seems by the fact that the four foundational papers of compressed sensing received, at the time of writing this manuscripts, more than 60000 citations. In contrast to the Nyquist-Shannon theorem that asserts that continuous band-limited signals can be perfectly reconstructed from samples taken at a rate of twice the highest frequency present in the signal of interest, compressed sensing allows lossless reconstruction from much lower number of samples given that certain natural conditions are satisfied.

This impressive improvement is due to the same phenomenon that makes modern image compression algorithms so successful: the sparsity of the signal to be recovered in a certain representation domain. And while the classic image processing flow starts with acquiring the fully sampled image, then feeding it to a compression algorithm that discards the vast majority of the data still allowing later a lossless decompression (e.g. JPEG or JPEG2000), the idea behind compressed sensing is that image acquisition can be made much more effective by fusing it with the compression step recording only the data we need later for decompression, hence the name compressed sensing. As will be discussed in this thesis, MRI possess natural sparse representation and due to physics of the nuclear magnetic resonance phenomenon, its acquisition process is dictated by Fourier transforms which makes it a perfect candidate for the use of compressed sensing machinery. Indeed, MRI was the first successful application of compressed sensing~\cite{lustig} and, since 2017, MRI scanner employing this technology are approved by the American Food and Drug Administration and commercially available~\cite{fda_siemens, fda_GE}.

% THIS SENTENCE WAS WRONG!
% Since MRI scanning operates directly in Fourier domain and all natural images tent to be sparse in the Fourier domain, compressed sensing is particularly effective in accelerating MRI acquisition.

As a trade-off for the acceleration in the scanning time, the posterior process of reconstructing the image from the measured data is much more involved compared to the standard one typically used when longer scans, i.e., fully sampled Fourier measurement, are performed. Therefore, a large amount of theory was developed since the introduction of compressed sensing to further improve the recovery of a high resolution image from the compressed representation. Ideas coming from high-dimensional statistics, non-linear optimization, harmonic analysis and signal processing came together in order to develop robust, stable and scalable reconstruction methods for compressed sensing. These ideas are particularly useful when applied to the MRI field since the minimization problems with its associated cost functions associated tend to be very challenging. Here, a few of those modern ideas will be discussed, in particular, \emph{accelerated proximal methods}, \emph{augmented Lagrangian methods} and \emph{iteratively reweighted least squares}

% number of studies investigates the possible solutions for the recovery of a high resolution image from the compressed representation. Most of these attempts starts from an already existing optimization method, defines a cost function which is hoped to lead to a more optimal solution, and maybe combines the resulted algorithm with some extra steps helping faster convergence or more exact recovery. But at the same time, new optimization algorithms or variants of existing algorithms are developed continuously, so another way to approach the problem is to try out new algorithms within the conventional frameworks.

\section{Julia Language}

Data scientists often find themselves in an very uncomfortable dilemma: Do they choose high-abstraction level dynamic languages that allows rapid prototyping and easy debugging, but fails to perform well in large-scale problems; or should they opt for high performance languages with lower abstraction level, spending much more time with the implementation. The rising popularity of Python in the recent decades also due to this problem. Python, however, instead of solving the problem, get around it by being merely a glu-language that connects high-performance libraries usually written in C or Fortran. The problem therefore is still present, as one still needs to write the performance-critical parts in a lower-level language. To give a better solution to that problem, Jeff Bezanson, Stefan Karpinski, Viral B. Shah, and Alan Edelman started to work on a brand new language in 2009. Their objective is bold, writing in their first blog post that they created Julia because...
\begin{quote}
...we want a language that's open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that's homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled.~\cite{bezanson_why_2012}
\end{quote}

As of 2020, we can say that their mission statement was not completely science fiction since the popularity of the language does not seems to decline ever since; in contrary, the number of users increases in a fast rate, especially in the academia and the field of data science. And as the objective of this thesis project involves heavy computing, the chosen language to implement the algorithms of interest is became Julia.

\section{Objective}

In this thesis work, we consider the classic results as well as the recent advances within of the compressed sensing framework and their application to real life MR imaging. In particular, we closely examine two recent publications presenting state-of-the-art solutions combining conventional techniques with novel ideas. Afterwards, we present our implementation of these algorithms along with the implementation of a recently invented algorithm from the family of iteratively least squares methods that previously have not been applied to MRI setting yet. Finally, we compare these algorithm with respect to reconstruction power from massively undersampled data and noise tolerance.

% ----------------------------------------------------
\section{Outline}
The structure of this thesis is as follows:

\paragraph{Chapter 2} introduces the reader to the core concepts of MRI, presents the hardware improvements of the last decades, and show the limits of further hardware improvements.

\paragraph{Chapter 3} summarizes the most important conditions and promises of compressed sensing framework, and then gives a quick overview of the most popular first order methods currently used in in the field.

\paragraph{Chapter 4} reviews three recent publications. Two of these papers proposes algorithms which are currently state-of-the-art algorithms for reconstruction of dynamic MR images, and the third one presents an algorithm that has stronger guarantees than the currently used solutions, and therefore a good candidate for reconstruction from even more reduced data.

\paragraph{Chapter 5} lists our contributions to the field presenting the results of numerical experiments.

\paragraph{Chapter 5} summarizes the project and articulates the future plans.

\clearpage % You need \clearpage at the end of every chapter to force images included in this chapter to be rendered in somewhere else