\chapter{Introduction}\label{chapter:introduction}

While the fast evolution of technology profoundly changed today's medicine, unarguably the medical imaging is of the fields which profited the most of the computation power recently became available. And as X-rays revolutionized medical treatments in the beginning of the 20th century, after its discovery by Wilhelm Conrad Röntgen,  the appearance of computer-aided imaging techniques such as computer tomography (CT), diagnostic ultrasonography, positron emission tomography (PET), and magnetic resonance imaging (MRI) opened a new horizon drastically increasing the resolution, allowing 3D imaging, providing reliable dynamic recordings, and enhancing images by automated post-processing \cite{mri_picturetoproton}. In the recent decades radiology evolved to be an interdisciplinary field involving, for instance, molecular biology, nuclear physics, applied mathematics, and computer science besides the classical medical fields such as anatomy, angiology, and cardiology.

\section{Magnetic Resonance Imaging}

In particular, MRI has revolutionized medical imaging and diagnostic process as we know it. Its versatility makes it fit a wide range of use cases. Compared to other imaging technologies, MRI demonstrates important advantages in many cases.
\begin{itemize}
    \item In contrast to X-ray, MRI doesn't use any ionizing radiation, and hence it is totally harmless to the patient. Also, MRI has enhanced resolution for soft tissues compared to any other modality, in particular neural tissue,  while X-rays are rather used for diagnosing bone degeneration, dislocation, fracture or some tissue infection. Furthermore, MRI allows 3D scans. Nevertheless, MRI scanners are slow and expensive compare to X-ray scan machines and therefore hardware and software improvements are necessary for its wider use.
    \item As CT scanning is based on X-rays, it shares the downside with X-rays, doctors need to evaluate the possible benefits of the scan and decide if it outweighs the potential complications of exposure to ionizing radiations. MRI, however, elicit this problem, although at the price of a elongated imaging process. Comparing the medical problems where these technologies are used, one can conclude that CT scan is very helpful in diagnosing severe injuries of the chest, head, spine or abdomen, particularly fractures, and it is commonly used to localize tumors. MRI often performs better at diagnosing problems in the joints, soft tissues, ligaments and tendons. When available, doctors use it frequently to scan the spine, brain, muscles, neck, breasts, and abdomen.
    \item MRI still does not have the portability, low cost, and real-time imaging speed without any harmful radiation of ultrasound technology, but ultrasound is mostly limited to 2D imaging (although 3D imaging is possible), have trouble penetrating bone, and even in absence of bone the depth of penetration is limited depending on the frequency of imaging. This is not the case for MRI technology.
    \item PET scans are particularly useful for functional imaging. For instance, it is used for identification of lapses in cognitive function, examination of cardiac failures, cancer screening and diagnosis, and finding an infection.  Nevertheless, PET image acquisition is even longer than MRI (especially, if we consider also the time while patients wait for the tracer to reach the targeted organ), it uses a radioactive substance as tracer, and it cannot scan tissues not absorbing the tracer making the localization of the source of the signal infeasible without any additional information. In order to solve the latter described limitation, one particularly promising combination is the join use of PET and MRI. This illustrates that MRI is a fundamental technology not only by itself but also when combined with other imaging modalities.
\end{itemize}
To sum up, MRI is a strong competitor to other imaging technologies, but it also have weaknesses, of which the costs and scanning time are the most remarkable. There are many methods to speed up measurements as it will be discussed later, but the construction cost and the hardware constraints limit the applicability of these efforts. The problem of slowness is even more apparent in case of dynamic images as motion of organs (e.g. heart or lung) can drastically degrade the image quality. To overcome that issue, mathematical solutions developed in the last twenty years such as \emph{parallel imaging} and \emph{compressive sensing} made high-resolution and fast images possible. This thesis concerns the second of these two powerful mathematical ideas.

\section{Compressed Sensing}

Among the many software techniques invented to improve image quality, compressed sensing (also known as compressive sensing, compressed sampling, and compressive sampling) is inevitably is one of the most impactful theoretical construction, introduced by Donoho, Candès, Romberg, and Tao in 2004~\cite{candes_robust_2006, donoho_compressed_2006, candes_nearoptimal_2006}. Such importance can be seems by the fact that the four foundational papers of compressive sensing received, at the time of writing this manuscripts, more than 60000 citations. In contrast to the Nyquist-Shannon theorem that asserts that continuous band-limited signals can be perfectly reconstructed from samples taken at a rate of twice the highest frequency present in the signal of interest, compressed sensing allows lossless reconstruction from much lower number of samples given that certain natural conditions are satisfied.

This impressive improvement is due to the same phenomenon that makes modern image compression algorithms so successful: the sparsity of the signal to be recovered in a certain representation domain. And while the classic image processing flow starts with acquiring the fully sampled image, then feeding it to a compression algorithm that discards the vast majority of the data still allowing later a lossless decompression (e.g. JPEG or JPEG2000), the idea behind compressed sensing is that image acquisition can be made much more effective by fusing it with the compression step recording only the data we need later for decompression, hence the name compressed sensing. As will be discussed in this thesis, MRI possess natural sparse representation and due to physics of the nuclear magnetic resonance phenomenon, its acquisition process is dictated by Fourier transforms which makes it a perfect candidate for the use of compressive sensing machinery. Indeed, MRI was the first successful application of compressive sensing \cite{lustig} and, since 2017, MRI scanner employing this technology are approved by the American Food and Drug Administration and commercially available \cite{fda_siemens, fda_GE}.

% THIS SENTENCE WAS WRONG!
% Since MRI scanning operates directly in Fourier domain and all natural images tent to be sparse in the Fourier domain, compressed sensing is particularly effective in accelerating MRI acquisition.

As a trade-off for the acceleration in the scanning time, the posterior process of reconstructing the image from the measured data is much more involved compared to the standard one typically used when longer scans, i.e., fully sampled Fourier measurement, are performed. Therefore, a large amount of theory was developed since the introduction of compressive sensing to further improve the recovery of a high resolution image from the compressed representation. Ideas coming from high-dimensional statistics, non-linear optimization, harmonic analysis and signal processing came together in order to develop robust, stable and scalable reconstruction methods for compressive sensing. These ideas are particularly useful when applied to the MRI field since the minimization problems with its associated cost functions associated tend to be very challenging. Here, a few of those modern ideas will be discussed, in particular, \emph{accelerated proximal methods} and \emph{iteratively reweighted least squares}

% number of studies investigates the possible solutions for the recovery of a high resolution image from the compressed representation. Most of these attempts starts from an already existing optimization method, defines a cost function which is hoped to lead to a more optimal solution, and maybe combines the resulted algorithm with some extra steps helping faster convergence or more exact recovery. But at the same time, new optimization algorithms or variants of existing algorithms are developed continuously, so another way to approach the problem is to try out new algorithms within the conventional frameworks.

\section{Julia Language}
\subsection{Objectives of The Language}
\subsection{Suitability to Our Task}

\section{Objective}

% ----------------------------------------------------
\section{Outline}
The summary of the chapters of the thesis work:

\paragraph{Chapter 2} This chapter describes something and here I summarize it in a couple sentences.

\paragraph{Chapter 3} This chapter describes something and here I summarize it in a couple sentences.

\paragraph{Chapter 4} This chapter describes something and here I summarize it in a couple sentences.

\paragraph{Chapter 5} This chapter describes something and here I summarize it in a couple sentences.

\clearpage % You need \clearpage at the end of every chapter to force images included in this chapter to be rendered in somewhere else