\chapter{Related Works}

%\section{Parallel Imaging}
%"Self-Calibrating Nonlinear Reconstruction Algorithms for Variable Density Sampling and Parallel Reception MRI" by Loubna El Gueddari, C. Lazarus, H Carrié, A. Vignaud, Ph Ciuciu

%SENSE and ESPIRiT for sensitivity map estimation

\iffalse

\color{red}
This chapter contains the text I copied from the project reports I've done with Wièm and Mahmoud, but this is far from ready!
\color{black}

\subsection{Sparse Formulation}


"Compressed Sensing MRI" (2008) by Michael Lustig, David L. Donoho, Juan M. Santos, and John M. Pauly --> "A look at how CS can improve on current imaging techniques"

"Sparse MRI: The Application of Compressed Sensing for Rapid MR Imaging" (2007) by Michael Lustig, David Donoho, and John M. Pauly

Having the measurement data that exhibit a required level of sparsity and incoherence, makes us able to apply optimization methods to reconstruct the original image from that undersampled measurement. The optimization problem is formulated in the following way: $$\textit{minimize} \lvert\lvert \Psi m \rvert\rvert_1 \textit{ such that } \lvert\lvert \mathcal{F}_u m - y \rvert\rvert_2 < \epsilon,$$
where $m$ image of interest, $\Psi$ a sparsifying transform, $\mathcal{F}_u$ undersampled Fourier transform, $y$ measured k-space data, $\epsilon$ threshold for expected noise level (controls fidelity of reconstruction). Minimizing $\lvert\lvert \Psi m \rvert\rvert_1$ promotes sparsity, constraint $\lvert\lvert \mathcal{F}_u m - y \rvert\rvert_2 < \epsilon$ enforces data consistency. When $\Psi$ is finite-differences operator (difference of neighbors), then we refer to $\lvert\lvert \Psi m \rvert\rvert_1$ as $TV(m)$. That operator is used many times as additional penalty: \textit{minimize} $\lvert\lvert \Psi m \rvert\rvert_1 + \alpha TV(m)$ \textit{such that} $\lvert\lvert \mathcal{F}_u m - y \rvert\rvert_2 < \epsilon$ where $\alpha$ trades $\Psi$ sparsity with finite-differences sparsity. Although there exist multiple methods to solve that constrained optimization problem, constrained optimization problems are considered to be difficult tasks to solve, so most of the times researchers try to find a way to convert the problem to unconstrained formulation. In our case, the Lagrangian form is a good solution to that problem:
$$\argmin_m \lvert\lvert\mathcal{F}_u m - y \rvert\rvert_2^2 + \lambda \lvert\lvert\Psi m \rvert\rvert_1,$$
where $\lambda$ is a regularization parameter that determines the trade-off between data consistency and sparsity. if $\lambda$ properly selected, then the two problem statements yield same results. $\lambda$ can be determined by trying many values and choosing one so that $\lvert\lvert\mathcal{F}_u m - y \rvert\rvert_2 \approx \epsilon$. Adding the total variance term and introducing the $f$ function to note the cost function, we get the following formula:
$$\argmin_m f(m)$$
$$\text{ where } f(m) := \lvert\lvert\mathcal{F}_u m - y \rvert\rvert_2^2 + \lambda \lvert\lvert\Psi m \rvert\rvert_1 + \alpha TV(m).$$

After these modifications, we can find many methods to minimize $f$ efficiently:
\begin{itemize}
    \item interior point methods
    \item projections onto convex sets
    \item homotopy
    \item iterative soft thresholding
    \item iteratively reweighted least squares
    \item nonlinear conjugate gradients (used in that article)
\end{itemize}
In the following, we attempt to briefly explain the motivation behind the \textit{nonlinear conjugate gradient} method, and show its mechanism.

\paragraph{Transform Sparsity}
 Sparse signals are signals that have a few non zero coefficients. Most natural signals like images and sounds are compressible i.e. can be represented with few nonzero coefficients in a certain basis without a big loss of information. While MR images are sparse in discrete cosine transform (DCT) and wavelet transform domains, angiograms, for instance, have already a sparse pixel representation.  One famous sparsifying transform is the Spatial Finite Differences which consists in computing the difference between neighboring pixels so the only non zero values are those of the pixels at the edges. Dynamic MR images are also highly compressible and have a sparse representation in the temporal Fourier Domain~\cite{parrish, lustig}.

\fi

\section{Proximal Optimized Gradient Method}
Why is it faster than FISTA, and why is it optimal

D. Kim and J. A. Fessler, “Optimized first-order methods for smooth convex minimization,” Math. Program., vol. 159, no. 1, pp. 81–107, Sep. 2016, doi: 10.1007/s10107-015-0949-3.

\section{Decompositions}

\subsection{Low rank and Sparse}
C. Y. Lin and J. A. Fessler, “Efficient Dynamic Parallel MRI Reconstruction for the Low-Rank Plus Sparse Model,” IEEE Transactions on Computational Imaging, vol. 5, no. 1, pp. 17–26, Mar. 2019, doi: 10.1109/TCI.2018.2882089.

J. A. Fessler, “Optimization methods for MR image reconstruction (long version),” arXiv:1903.03510 [eess, math], Jun. 2019.

\subsection{Multiscale}

Ong's dissertation: “Low Dimensional Methods for High Dimensional Magnetic Resonance Imaging,” 2018.

F. Ong et al., “Extreme MRI: Large-Scale Volumetric Dynamic Imaging from Continuous Non-Gated Acquisitions,” arXiv:1909.13482 [physics], Dec. 2019.

Differences between Ong's dissertation and his "extreme MRI" preprint paper

\section{IRSL}
Iteratively reweighted Least Squares method

C. Kümmerle and C. M. Verdun, “Denoising and Completion of Structured Low-Rank Matrices via Iteratively Reweighted Least Squares,” arXiv:1811.07472 [cs, math], Nov. 2018.

Henry Adams, Lara Kassab, and Deanna Needell "An Iterative Method for Structured Matrix Completion"

\clearpage % You need \clearpage at the end of every chapter to force images included in this chapter to be rendered in somewhere else